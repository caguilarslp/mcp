# TASK-105: CRITICAL - LLM Security Migration to Backend

**Date**: 2025-06-25  
**Priority**: CRITICAL üö®  
**Status**: URGENT  
**Category**: Security

## üö® **PROBLEMA CR√çTICO IDENTIFICADO**

### **VULNERABILIDADES ACTUALES**:
- ‚ùå **API Keys expuestas** en frontend bundle
- ‚ùå **`dangerouslyAllowBrowser: true`** en providers
- ‚ùå **Variables VITE_** con secrets en cliente
- ‚ùå **Sin rate limiting** de costos LLM
- ‚ùå **Sin control de acceso** a providers

### **RIESGO**: 
- **Costos descontrolados** - Usuarios pueden extraer keys
- **Abuse potential** - Keys usadas externamente
- **Security breach** - Acceso no autorizado a LLMs

## üéØ **OBJETIVO**

Migrar **TODA la l√≥gica LLM al backend** para seguridad m√°xima:
- ‚úÖ **API Keys solo en servidor**
- ‚úÖ **Rate limiting por usuario** 
- ‚úÖ **Control de costos**
- ‚úÖ **Auditor√≠a completa**
- ‚úÖ **Frontend seguro** (sin secrets)

## üèóÔ∏è **IMPLEMENTACI√ìN**

### **Phase 1: Backend LLM Service** (2 d√≠as)

#### **Nueva Estructura**:
```python
src/api/services/llm/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ llm_service.py              # Core service
‚îú‚îÄ‚îÄ providers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ anthropic_provider.py   # Server-side Anthropic
‚îÇ   ‚îú‚îÄ‚îÄ openai_provider.py      # Server-side OpenAI
‚îÇ   ‚îî‚îÄ‚îÄ google_provider.py      # Server-side Google
‚îú‚îÄ‚îÄ context_builder.py          # Market context
‚îú‚îÄ‚îÄ prompt_templates.py         # Prompt management
‚îú‚îÄ‚îÄ security.py                 # Rate limiting, sanitization
‚îî‚îÄ‚îÄ streaming.py                # SSE streaming
```

#### **Core LLM Service**:
```python
# src/api/services/llm/llm_service.py
class LLMService:
    def __init__(self):
        # API Keys solo en servidor
        self.providers = {
            'anthropic': AnthropicProvider(os.getenv('ANTHROPIC_API_KEY')),
            'openai': OpenAIProvider(os.getenv('OPENAI_API_KEY')),
            'google': GoogleProvider(os.getenv('GOOGLE_API_KEY'))
        }
        
    async def analyze_market(self, query: str, symbol: str, user_id: str):
        # Rate limiting por usuario
        await self.check_rate_limit(user_id)
        
        # Build context from our indicators
        context = await self.build_market_context(symbol)
        
        # Select optimal provider
        provider = self.select_provider(query, context)
        
        # Execute analysis
        result = await provider.analyze(query, context)
        
        # Log usage for billing/monitoring
        await self.log_usage(user_id, result.tokens_used, result.cost)
        
        return result
```

#### **Security Layer**:
```python
# src/api/services/llm/security.py
class RateLimiter:
    async def check_limit(self, user_id: str, action: str):
        # 50 requests per hour per user
        # $10 max cost per user per day
        # Block if exceeded
        
class DataSanitizer:
    def sanitize_context(self, context: dict) -> dict:
        # Remove sensitive data before sending to LLM
        # No user positions, balances, API keys
```

### **Phase 2: Secure API Endpoints** (1 d√≠a)

```python
# src/api/routers/chat.py
@router.post("/chat/analyze")
async def analyze_market(
    request: ChatRequest,
    api_key: APIKeyInfo = Depends(verify_api_key)
):
    result = await llm_service.analyze_market(
        query=request.message,
        symbol=request.symbol,
        user_id=api_key.id
    )
    return result

@router.post("/chat/stream")
async def stream_analysis(
    request: ChatRequest,
    api_key: APIKeyInfo = Depends(verify_api_key)
):
    return StreamingResponse(
        llm_service.stream_analysis(request, api_key.id),
        media_type="text/event-stream"
    )
```

### **Phase 3: Frontend Security Cleanup** (1 d√≠a)

#### **Remove Dangerous Code**:
```typescript
// ‚ùå REMOVE THESE FILES:
// - app/src/services/llm/providers/anthropic.ts
// - app/src/services/llm/providers/openai.ts  
// - app/src/services/llm/providers/google.ts
// - app/src/services/llm/demo.ts (with API keys)

// ‚ùå REMOVE FROM .env:
// - VITE_ANTHROPIC_API_KEY
// - VITE_OPENAI_API_KEY
// - VITE_GOOGLE_API_KEY
```

#### **New Secure Frontend**:
```typescript
// ‚úÖ NEW: Secure chat service
class ChatService {
  async sendMessage(message: string, symbol: string): Promise<ChatResponse> {
    // Only call our secure backend
    return await this.api.post('/api/v1/chat/analyze', {
      message,
      symbol
    });
  }
  
  async streamMessage(message: string, symbol: string): Promise<EventSource> {
    // Stream from our secure backend
    return new EventSource(`/api/v1/chat/stream?message=${message}&symbol=${symbol}`);
  }
}
```

## üìã **IMPLEMENTATION CHECKLIST**

### **Day 1: Backend Core**
- [ ] Create `src/api/services/llm/` structure
- [ ] Implement `LLMService` class
- [ ] Create server-side providers (Anthropic, OpenAI, Google)
- [ ] Implement rate limiting and security
- [ ] Basic market context builder

### **Day 2: API Endpoints**
- [ ] Create `/api/v1/chat/analyze` endpoint
- [ ] Create `/api/v1/chat/stream` endpoint  
- [ ] Implement SSE streaming
- [ ] Add request/response models
- [ ] Integration testing

### **Day 3: Frontend Security**
- [ ] Remove all LLM providers from frontend
- [ ] Remove API keys from environment
- [ ] Refactor ChatService to use backend
- [ ] Update components to use new service
- [ ] Remove `dangerouslyAllowBrowser` configurations

### **Day 4: Testing & Monitoring**
- [ ] End-to-end testing
- [ ] Cost monitoring setup
- [ ] Usage logging and analytics
- [ ] Security testing
- [ ] Performance optimization

## üîê **SECURITY FEATURES**

### **Rate Limiting**:
```python
# Per user limits
LIMITS = {
    'requests_per_hour': 50,
    'requests_per_day': 200,
    'cost_per_day_usd': 10.00,
    'tokens_per_request': 4000
}
```

### **Data Sanitization**:
```python
def sanitize_context(context):
    # Remove sensitive fields
    safe_context = {
        'symbol': context['symbol'],
        'price': context['current_price'],
        'indicators': context['indicators'],
        # NO user data, positions, balances
    }
    return safe_context
```

### **Audit Logging**:
```python
await log_llm_usage({
    'user_id': user_id,
    'query': query[:100],  # First 100 chars only
    'provider': provider_name,
    'tokens_used': tokens,
    'cost': cost,
    'timestamp': datetime.utcnow(),
    'success': True
})
```

## üìä **BENEFITS**

### **Security**:
- ‚úÖ **Zero API key exposure**
- ‚úÖ **Cost control per user**
- ‚úÖ **Rate limiting enforcement**
- ‚úÖ **Complete audit trail**
- ‚úÖ **Data sanitization**

### **Performance**:
- ‚úÖ **Server-side caching**
- ‚úÖ **Connection pooling**
- ‚úÖ **Smart provider routing**
- ‚úÖ **Optimized context building**

### **Business**:
- ‚úÖ **Cost predictability**
- ‚úÖ **Usage analytics**
- ‚úÖ **Scalable architecture**
- ‚úÖ **Enterprise ready**

## üö® **IMMEDIATE ACTIONS**

### **TODAY**:
1. **Remove API keys** from all frontend files
2. **Disable LLM features** temporarily
3. **Start backend implementation**

### **THIS WEEK**:
1. **Complete backend LLM service**
2. **Secure API endpoints**
3. **Frontend security cleanup**
4. **Testing and monitoring**

## üìà **SUCCESS METRICS**

- [ ] **Zero API keys** in frontend bundle
- [ ] **Rate limiting** working (429 errors for abuse)
- [ ] **Cost tracking** per user
- [ ] **Audit logs** for all LLM usage
- [ ] **Performance** <2s response time
- [ ] **Security scan** passes (no exposed secrets)

---

**Status**: üö® **CRITICAL SECURITY ISSUE**  
**Timeline**: 4 days maximum  
**Dependencies**: None - can start immediately  
**Impact**: **FOUNDATIONAL** - Enables secure LLM features 